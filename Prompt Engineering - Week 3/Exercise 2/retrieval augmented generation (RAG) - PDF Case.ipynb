{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T22:16:34.584666Z",
     "iopub.status.busy": "2025-06-20T22:16:34.584367Z",
     "iopub.status.idle": "2025-06-20T22:18:36.557598Z",
     "shell.execute_reply": "2025-06-20T22:18:36.555548Z",
     "shell.execute_reply.started": "2025-06-20T22:16:34.584639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalar las librerÃ­as necesarias para el proyecto RAG\n",
    "# - transformers: modelos de NLP pre-entrenados\n",
    "# - datasets: acceso a datasets de HuggingFace\n",
    "# - accelerate: optimizaciÃ³n para inferencia rÃ¡pida\n",
    "# - faiss-cpu: bÃºsqueda similar de vectores (base de datos vectorial)\n",
    "# - sentence-transformers: modelos para generar embeddings de oraciones\n",
    "# - pymupdf: extracciÃ³n de texto de archivos PDF\n",
    "!pip install -q transformers datasets accelerate faiss-cpu sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG): TeorÃ­a y Conceptos\n",
    "\n",
    "## Â¿QuÃ© es RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** es una arquitectura que combina dos componentes principales:\n",
    "1. **Retrieval (RecuperaciÃ³n)**: Extrae informaciÃ³n relevante de una base de datos o documentos\n",
    "2. **Generation (GeneraciÃ³n)**: Usa un modelo generativo (LLM) para crear respuestas basadas en la informaciÃ³n recuperada\n",
    "\n",
    "En lugar de que el LLM genere respuestas Ãºnicamente basadas en su conocimiento interno (entrenamiento), RAG le proporciona contexto externo relevante, mejorando la precisiÃ³n y evitando alucinaciones.\n",
    "\n",
    "## ProblemÃ¡tica que Resuelve\n",
    "\n",
    "Los modelos de lenguaje grandes (LLMs) tienen dos limitaciones principales:\n",
    "- **Conocimiento obsoleto**: No pueden actualizarse con informaciÃ³n nueva despuÃ©s del entrenamiento\n",
    "- **Alucinaciones**: Generan informaciÃ³n ficticia cuando no tienen conocimiento sobre un tema\n",
    "- **Imposibilidad de acceder a informaciÃ³n privada**: No pueden consultar documentos o bases de datos especÃ­ficas de una organizaciÃ³n\n",
    "\n",
    "RAG resuelve estos problemas al hacer que el modelo genere respuestas basadas en documentos reales.\n",
    "\n",
    "## Componentes Principales\n",
    "\n",
    "### 1. **Chunking (FragmentaciÃ³n)**\n",
    "- Divide documentos largos en fragmentos manejables (chunks)\n",
    "- Permite procesamiento eficiente sin perder contexto\n",
    "- TÃ­picamente: 100-500 tokens por chunk\n",
    "\n",
    "### 2. **Embedding (VectorizaciÃ³n)**\n",
    "- Convierte texto en vectores numÃ©ricos (embeddings) que capturan el significado semÃ¡ntico\n",
    "- Permite comparar similitud entre textos\n",
    "- Modelos populares: BERT, Sentence-Transformers, OpenAI Embeddings\n",
    "\n",
    "### 3. **Vector Database / Ãndice de BÃºsqueda**\n",
    "- Almacena embeddings para bÃºsqueda rÃ¡pida\n",
    "- Herramientas: FAISS, Pinecone, Weaviate, Elasticsearch\n",
    "- Permite encontrar fragmentos similares a la pregunta del usuario\n",
    "\n",
    "### 4. **Retrieval (BÃºsqueda SemÃ¡ntica)**\n",
    "- Convierte la pregunta en un embedding\n",
    "- Busca los K fragmentos mÃ¡s similares en la base de datos vectorial\n",
    "- Ordena por relevancia usando distancia euclidiana o similitud del coseno\n",
    "\n",
    "### 5. **GeneraciÃ³n con Contexto**\n",
    "- Crea un prompt que incluye:\n",
    "  - El contexto recuperado\n",
    "  - La pregunta del usuario\n",
    "- El LLM genera la respuesta basada en este prompt mejorado\n",
    "\n",
    "## Flujo del Pipeline RAG\n",
    "\n",
    "```\n",
    "Documento PDF\n",
    "    â†“\n",
    "ExtracciÃ³n de Texto\n",
    "    â†“\n",
    "Chunking (FragmentaciÃ³n)\n",
    "    â†“\n",
    "Embedding (VectorizaciÃ³n)\n",
    "    â†“\n",
    "Almacenamiento en FAISS\n",
    "    â†“\n",
    "Pregunta del Usuario\n",
    "    â†“\n",
    "BÃºsqueda SemÃ¡ntica â†’ Recupera Top-3 Fragmentos Relevantes\n",
    "    â†“\n",
    "ConstrucciÃ³n del Prompt (Contexto + Pregunta)\n",
    "    â†“\n",
    "LLM Generativo\n",
    "    â†“\n",
    "Respuesta Fundamentada\n",
    "```\n",
    "\n",
    "## Ventajas de RAG\n",
    "\n",
    "âœ… **Respuestas mÃ¡s precisas**: Basadas en documentos reales, no en alucinaciones\n",
    "âœ… **ActualizaciÃ³n dinÃ¡mica**: Puede incorporar nuevos documentos sin reentrenar\n",
    "âœ… **Trazabilidad**: Puedes saber quÃ© fuentes inspiraron la respuesta\n",
    "âœ… **InformaciÃ³n privada**: Accede a documentos confidenciales de la organizaciÃ³n\n",
    "âœ… **ReducciÃ³n de costos**: Evita usar modelos mÃ¡s grandes, mantiene calidad\n",
    "\n",
    "## DesafÃ­os\n",
    "\n",
    "âš ï¸ **Calidad del chunking**: Fragmentos mal divididos pueden perder contexto\n",
    "âš ï¸ **Relevancia**: El retriever podrÃ­a no encontrar los fragmentos necesarios\n",
    "âš ï¸ **Latencia**: Requiere bÃºsqueda en vector database antes de generaciÃ³n\n",
    "âš ï¸ **Mantenimiento**: Necesita actualizar embeddings cuando cambian documentos\n",
    "\n",
    "## Casos de Uso\n",
    "\n",
    "- ğŸ“„ **BÃºsqueda en documentos empresariales**: Intranets, manuales, polÃ­ticas\n",
    "- ğŸ’¼ **Soporte al cliente**: Bases de conocimiento de productos\n",
    "- ğŸ“š **AnÃ¡lisis acadÃ©mico**: BÃºsqueda en literatura cientÃ­fica\n",
    "- ğŸ¥ **Medicina**: Acceso a literatura mÃ©dica y guÃ­as clÃ­nicas\n",
    "- âš–ï¸ **Legal**: AnÃ¡lisis de contratos y jurisprudencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Chunk pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:36.560401Z",
     "iopub.status.busy": "2025-06-20T22:18:36.559971Z",
     "iopub.status.idle": "2025-06-20T22:18:36.712642Z",
     "shell.execute_reply": "2025-06-20T22:18:36.711412Z",
     "shell.execute_reply.started": "2025-06-20T22:18:36.560351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF - librerÃ­a para leer archivos PDF\n",
    "\n",
    "def extract_text_from_pdf_by_sentences(path):\n",
    "    \"\"\"\n",
    "    Extrae todo el texto de un archivo PDF.\n",
    "    \n",
    "    Args:\n",
    "        path (str): ruta al archivo PDF\n",
    "        \n",
    "    Returns:\n",
    "        str: texto completo extraÃ­do del PDF (todas las pÃ¡ginas juntas)\n",
    "    \"\"\"\n",
    "    # Abre el archivo PDF\n",
    "    doc = fitz.open(path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    # Itera sobre cada pÃ¡gina del PDF\n",
    "    for page in doc:\n",
    "        # Extrae el texto de cada pÃ¡gina y lo aÃ±ade al texto completo\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk pdf into paragraphs or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:36.716106Z",
     "iopub.status.busy": "2025-06-20T22:18:36.715714Z",
     "iopub.status.idle": "2025-06-20T22:18:39.262626Z",
     "shell.execute_reply": "2025-06-20T22:18:39.261301Z",
     "shell.execute_reply.started": "2025-06-20T22:18:36.716077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Descargar el tokenizador de NLTK (punto es necesario para dividir oraciones)\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def chunk_text(text, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos pequeÃ±os (chunks) para procesar con el modelo.\n",
    "    \n",
    "    Args:\n",
    "        text (str): texto a dividir\n",
    "        max_tokens (int): nÃºmero mÃ¡ximo de tokens por chunk (default: 200)\n",
    "        \n",
    "    Returns:\n",
    "        list: lista de fragmentos de texto\n",
    "    \"\"\"\n",
    "    # Importar tokenizador de transformers para contar tokens de forma consistente\n",
    "    from transformers import AutoTokenizer\n",
    "    # Usar el tokenizador del modelo all-MiniLM-L6-v2 (22 millones de parÃ¡metros)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Dividir el texto en oraciones individuales\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    # Procesar cada oraciÃ³n\n",
    "    for sentence in sentences:\n",
    "        # Contar cuÃ¡ntos tokens tiene la oraciÃ³n actual\n",
    "        length = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        # Si la oraciÃ³n cabe en el chunk actual sin exceder max_tokens\n",
    "        if current_length + length <= max_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "            current_length += length\n",
    "        else:\n",
    "            # Si no cabe, guardar el chunk actual y comenzar uno nuevo\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_length = length\n",
    "    \n",
    "    # Agregar el Ãºltimo chunk si hay contenido pendiente\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed chunks with sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:39.264007Z",
     "iopub.status.busy": "2025-06-20T22:18:39.263595Z",
     "iopub.status.idle": "2025-06-20T22:19:31.445693Z",
     "shell.execute_reply": "2025-06-20T22:19:31.444499Z",
     "shell.execute_reply.started": "2025-06-20T22:18:39.263981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 22:18:57.843009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750457938.094156      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750457938.171841      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb24592f22459681a34befe5b4d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22ac0dc63774a9b8cace90e524caea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c24d8ce7b545a3ab3282735b7e7414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c80a9e48bb4417b3481741be9f7e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d859973c54e42378b26e8186465b7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2326e009ac244b199900689436927684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeff090c8d64b7c9f8fd4a7a2c1c249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743d0d7e1dc4e99ac799edda7ed12a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d560db5954024bd9f0b3e658f16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2848d3cd2ce1470ba3bc0cae3418fab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef2dcd8be8c496ea2fe24cf19ecd0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo pre-entrenado para generar embeddings\n",
    "# Este modelo convierte texto en vectores numÃ©ricos de 384 dimensiones\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Convierte los fragmentos de texto en vectores numÃ©ricos (embeddings).\n",
    "    Estos vectores capturan el significado semÃ¡ntico del texto.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): lista de fragmentos de texto\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: matriz de embeddings, una fila por chunk\n",
    "    \"\"\"\n",
    "    # Codificar cada chunk en un vector de embedding\n",
    "    # show_progress_bar=True muestra una barra de progreso durante el procesamiento\n",
    "    return model.encode(chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store embeddings with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:19:31.447532Z",
     "iopub.status.busy": "2025-06-20T22:19:31.446816Z",
     "iopub.status.idle": "2025-06-20T22:19:31.497575Z",
     "shell.execute_reply": "2025-06-20T22:19:31.496496Z",
     "shell.execute_reply.started": "2025-06-20T22:19:31.447504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Construye un Ã­ndice FAISS (base de datos vectorial) para bÃºsqueda rÃ¡pida.\n",
    "    FAISS permite encontrar embeddings similares de manera eficiente.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): matriz de embeddings (n_chunks x dimensiÃ³n)\n",
    "        \n",
    "    Returns:\n",
    "        faiss.Index: Ã­ndice FAISS configurado y poblado con los embeddings\n",
    "    \"\"\"\n",
    "    # Obtener la dimensiÃ³n del embedding (384 para all-MiniLM-L6-v2)\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Crear un Ã­ndice de bÃºsqueda con distancia L2 (Euclidiana)\n",
    "    # IndexFlatL2 es una bÃºsqueda exacta (sin aproximaciones)\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    \n",
    "    # Agregar los embeddings al Ã­ndice\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:20:14.785103Z",
     "iopub.status.busy": "2025-06-20T22:20:14.784611Z",
     "iopub.status.idle": "2025-06-20T22:21:53.874292Z",
     "shell.execute_reply": "2025-06-20T22:21:53.872985Z",
     "shell.execute_reply.started": "2025-06-20T22:20:14.785072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5899c9232d174edaa02094ced7f8497e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ba25867f0c4b158b0fbf229e40a06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c136b44fb7bb4bf8a3cd7b7aa25bf882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d252ac5d9a424999ff9a3117428992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aec6999a83f46ab85122151bc8ba6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c821ea8263a41c8990aec17712189c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7850e84deb934d00892795df729b559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c3a278c1b4893a05f62998a7c443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd59e62f3cdd41bb80b7c08443ae231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27523318836d457b93a0dc4f4bf3ef7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a601715b39494978b4f6005fb371f958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a57473ce2343a9b6a651ee2f88d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "# Cargar credenciales de HuggingFace desde archivo JSON\n",
    "# Este token permite acceder a modelos privados o con cuota limitada\n",
    "with open(\"/kaggle/input/autenti/AUTH nn.json\", \"r\") as f:\n",
    "    token_data = json.load(f)\n",
    "\n",
    "HF_TOKEN = token_data[\"API_KEY\"]\n",
    "\n",
    "# Autenticarse en HuggingFace Hub\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# InformaciÃ³n del modelo a usar:\n",
    "# - 7 mil millones de parÃ¡metros\n",
    "# - Soporta mÃºltiples idiomas: inglÃ©s, espaÃ±ol, alemÃ¡n, italiano, francÃ©s, portuguÃ©s, etc.\n",
    "# - Datos de entrenamiento: Common Crawl, cÃ³digo abierto de GitHub, Wikipedia, Project Gutenberg\n",
    "#   Corpus multilingÃ¼es (OSCAR, C4, The Pile), Q&A tÃ©cnico, foros (Reddit, StackExchange)\n",
    "\n",
    "# Cargar el pipeline para generaciÃ³n de texto\n",
    "# Usando Nous-Hermes-2-Mistral-7B-DPO (o Mistral-7B-Instruct-v0.1 como alternativa)\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\")\n",
    "\n",
    "def retrieve_and_answer(query, chunks, index, embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Implementa el pipeline RAG (Retrieval-Augmented Generation):\n",
    "    1. Recupera los fragmentos mÃ¡s relevantes del documento\n",
    "    2. Usa el LLM para responder basado en el contexto recuperado\n",
    "    \n",
    "    Args:\n",
    "        query (str): pregunta del usuario\n",
    "        chunks (list): fragmentos de texto del documento\n",
    "        index (faiss.Index): Ã­ndice FAISS para bÃºsqueda\n",
    "        embeddings (np.ndarray): embeddings de los fragmentos\n",
    "        top_k (int): nÃºmero de fragmentos relevantes a recuperar (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        str: respuesta generada por el modelo\n",
    "    \"\"\"\n",
    "    # Paso 1: Convertir la pregunta en un embedding\n",
    "    query_vec = model.encode([query])\n",
    "    \n",
    "    # Paso 2: Buscar los top_k fragmentos mÃ¡s similares en FAISS\n",
    "    # distances: distancias a los fragmentos encontrados\n",
    "    # indices: Ã­ndices de los fragmentos mÃ¡s similares\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    \n",
    "    # Paso 3: Recuperar el texto de los fragmentos relevantes\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "    \n",
    "    # Paso 4: Construir el prompt combinando contexto + pregunta\n",
    "    context = \"\\n--------------------------------------------------\\n\".join(retrieved_texts)\n",
    "    prompt = f\"Answer this question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Paso 5: Generar respuesta usando el LLM\n",
    "    # max_new_tokens: limitar la longitud de la respuesta\n",
    "    # do_sample=True: usar muestreo (mÃ¡s creativo) en lugar de greedy decoding\n",
    "    response = qa_pipeline(prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:22:44.244429Z",
     "iopub.status.busy": "2025-06-20T22:22:44.243504Z",
     "iopub.status.idle": "2025-06-20T22:22:45.734170Z",
     "shell.execute_reply": "2025-06-20T22:22:45.732828Z",
     "shell.execute_reply.started": "2025-06-20T22:22:44.244378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9ab24f2d214f3db022199bf6874911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ruta al archivo PDF a procesar\n",
    "pdf_path = \"/kaggle/input/dataset/WWII_Key_Events_2pages.pdf\"\n",
    "\n",
    "# Ejecutar el pipeline RAG completo:\n",
    "\n",
    "# 1. Extraer texto del PDF\n",
    "text = extract_text_from_pdf_by_sentences(pdf_path)\n",
    "\n",
    "# 2. Dividir el texto en fragmentos manejables\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# 3. Generar embeddings para cada fragmento\n",
    "embeddings = embed_chunks(chunks)\n",
    "\n",
    "# 4. Construir el Ã­ndice FAISS para bÃºsqueda rÃ¡pida\n",
    "index = build_faiss_index(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:27:23.247957Z",
     "iopub.status.busy": "2025-06-20T22:27:23.247546Z",
     "iopub.status.idle": "2025-06-20T22:30:23.813381Z",
     "shell.execute_reply": "2025-06-20T22:30:23.811810Z",
     "shell.execute_reply.started": "2025-06-20T22:27:23.247909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e98ec63e594b18a8b8199dd44fc58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time in minutes: 3.0092994372049966\n",
      "Answer this question based on the context below:\n",
      "\n",
      "Context:\n",
      "Operation Barbarossa (1941)\n",
      "In June 1941, Nazi Germany launched Operation Barbarossa, a massive invasion of the Soviet\n",
      "Union. This marked a major escalation of the war and opened the Eastern Front, which became the\n",
      "largest and bloodiest theater of conflict in World War II. Despite early successes, German forces\n",
      "were eventually halted outside Moscow and faced fierce Soviet resistance, especially in Stalingrad. Pearl Harbor (1941)\n",
      "On December 7, 1941, the Japanese navy launched a surprise attack on the U.S. naval base at\n",
      "Pearl Harbor in Hawaii. The attack killed over 2,400 Americans and led the United States to declare\n",
      "war on Japan the following day. This event brought the United States into the global conflict, which\n",
      "shifted the balance of power among the Allied and Axis forces. Battle of Stalingrad (1942-1943)\n",
      "One of the turning points of the war, the Battle of Stalingrad saw Soviet forces defending the city\n",
      "from a brutal German siege.\n",
      "--------------------------------------------------\n",
      "Key Events of World War II\n",
      "Invasion of Poland (1939)\n",
      "On September 1, 1939, Germany invaded Poland, triggering the beginning of World War II. This act\n",
      "of aggression led Britain and France to declare war on Germany just two days later. The German\n",
      "army used a military strategy known as 'blitzkrieg', or lightning war, involving rapid attacks with tanks\n",
      "and aircraft to overwhelm the Polish defenses. Poland was quickly defeated, and this marked the\n",
      "start of a devastating global conflict. Fall of France (1940)\n",
      "In May 1940, Germany launched an invasion of France and the Low Countries. Within six weeks,\n",
      "German forces had bypassed the heavily fortified Maginot Line and captured Paris. The swift defeat\n",
      "shocked the world and left Britain to stand alone against Nazi Germany for much of the next year. France was divided into an occupied zone and a nominally independent Vichy government that\n",
      "collaborated with the Nazis.\n",
      "--------------------------------------------------\n",
      "After months of fierce combat and enormous casualties, the Soviet\n",
      "Union encircled and defeated the German 6th Army. This defeat marked the beginning of a major\n",
      "Soviet offensive pushing westward. D-Day - Normandy Invasion (1944)\n",
      "On June 6, 1944, Allied forces launched the largest amphibious invasion in history, landing on the\n",
      "beaches of Normandy, France. Codenamed Operation Overlord, this marked the beginning of the\n",
      "liberation of Western Europe from Nazi control. Despite strong German resistance, the Allies\n",
      "successfully established a foothold and began pushing inland. Fall of Berlin and Hitler's Death (1945)\n",
      "By April 1945, Soviet troops had reached Berlin. After weeks of intense street fighting, the city fell. Adolf Hitler committed suicide in his bunker on April 30. Germany surrendered shortly after, bringing\n",
      "an end to the war in Europe. Atomic Bombs and Japan's Surrender (1945)\n",
      "In August 1945, the United States dropped atomic bombs on the Japanese cities of Hiroshima and\n",
      "Nagasaki.\n",
      "\n",
      "Question: What caused the United States to enter World War II?\n",
      "Answer: The attack on the U.S. naval base at Pearl Harbor by the Japanese navy caused the United States to declare war on Japan and enter World War II.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Pregunta a responder basada en el documento PDF\n",
    "question = \"What caused the United States to enter World War II?\"\n",
    "\n",
    "# Registrar el tiempo de inicio\n",
    "initial = time.time()\n",
    "\n",
    "# Ejecutar el pipeline RAG para obtener la respuesta\n",
    "answer = retrieve_and_answer(question, chunks, index, embeddings)\n",
    "\n",
    "# Registrar el tiempo final\n",
    "final = time.time()\n",
    "\n",
    "# Mostrar el tiempo de ejecuciÃ³n en minutos\n",
    "print(\"time in minutes:\", (final-initial) / 60)\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "1. Ask 5 questions and evaluate model answers qualitatively. Add brief comments.\n",
    "\n",
    "2. Calculate BLEU score for the answers. Comment on the results.\n",
    "\n",
    "3. Upload your own PDF, ask 5 new questions, and comment on the new answers.\n",
    "\n",
    "4. Calculate BLEU score again and compare it with the previous results."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7642611,
     "sourceId": 12135899,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7686248,
     "sourceId": 12211360,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
