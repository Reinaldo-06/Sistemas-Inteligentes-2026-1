{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T22:16:34.584666Z",
     "iopub.status.busy": "2025-06-20T22:16:34.584367Z",
     "iopub.status.idle": "2025-06-20T22:18:36.557598Z",
     "shell.execute_reply": "2025-06-20T22:18:36.555548Z",
     "shell.execute_reply.started": "2025-06-20T22:16:34.584639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalar las librerías necesarias para el proyecto RAG\n",
    "# - transformers: modelos de NLP pre-entrenados\n",
    "# - datasets: acceso a datasets de HuggingFace\n",
    "# - accelerate: optimización para inferencia rápida\n",
    "# - faiss-cpu: búsqueda similar de vectores (base de datos vectorial)\n",
    "# - sentence-transformers: modelos para generar embeddings de oraciones\n",
    "# - pymupdf: extracción de texto de archivos PDF\n",
    "!pip install -q transformers datasets accelerate faiss-cpu sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Chunk pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:36.560401Z",
     "iopub.status.busy": "2025-06-20T22:18:36.559971Z",
     "iopub.status.idle": "2025-06-20T22:18:36.712642Z",
     "shell.execute_reply": "2025-06-20T22:18:36.711412Z",
     "shell.execute_reply.started": "2025-06-20T22:18:36.560351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF - librería para leer archivos PDF\n",
    "\n",
    "def extract_text_from_pdf_by_sentences(path):\n",
    "    \"\"\"\n",
    "    Extrae todo el texto de un archivo PDF.\n",
    "    \n",
    "    Args:\n",
    "        path (str): ruta al archivo PDF\n",
    "        \n",
    "    Returns:\n",
    "        str: texto completo extraído del PDF (todas las páginas juntas)\n",
    "    \"\"\"\n",
    "    # Abre el archivo PDF\n",
    "    doc = fitz.open(path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    # Itera sobre cada página del PDF\n",
    "    for page in doc:\n",
    "        # Extrae el texto de cada página y lo añade al texto completo\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk pdf into paragraphs or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:36.716106Z",
     "iopub.status.busy": "2025-06-20T22:18:36.715714Z",
     "iopub.status.idle": "2025-06-20T22:18:39.262626Z",
     "shell.execute_reply": "2025-06-20T22:18:39.261301Z",
     "shell.execute_reply.started": "2025-06-20T22:18:36.716077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Descargar el tokenizador de NLTK (punto es necesario para dividir oraciones)\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def chunk_text(text, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos pequeños (chunks) para procesar con el modelo.\n",
    "    \n",
    "    Args:\n",
    "        text (str): texto a dividir\n",
    "        max_tokens (int): número máximo de tokens por chunk (default: 200)\n",
    "        \n",
    "    Returns:\n",
    "        list: lista de fragmentos de texto\n",
    "    \"\"\"\n",
    "    # Importar tokenizador de transformers para contar tokens de forma consistente\n",
    "    from transformers import AutoTokenizer\n",
    "    # Usar el tokenizador del modelo all-MiniLM-L6-v2 (22 millones de parámetros)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Dividir el texto en oraciones individuales\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    # Procesar cada oración\n",
    "    for sentence in sentences:\n",
    "        # Contar cuántos tokens tiene la oración actual\n",
    "        length = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        # Si la oración cabe en el chunk actual sin exceder max_tokens\n",
    "        if current_length + length <= max_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "            current_length += length\n",
    "        else:\n",
    "            # Si no cabe, guardar el chunk actual y comenzar uno nuevo\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_length = length\n",
    "    \n",
    "    # Agregar el último chunk si hay contenido pendiente\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed chunks with sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:18:39.264007Z",
     "iopub.status.busy": "2025-06-20T22:18:39.263595Z",
     "iopub.status.idle": "2025-06-20T22:19:31.445693Z",
     "shell.execute_reply": "2025-06-20T22:19:31.444499Z",
     "shell.execute_reply.started": "2025-06-20T22:18:39.263981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 22:18:57.843009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750457938.094156      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750457938.171841      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb24592f22459681a34befe5b4d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22ac0dc63774a9b8cace90e524caea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c24d8ce7b545a3ab3282735b7e7414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c80a9e48bb4417b3481741be9f7e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d859973c54e42378b26e8186465b7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2326e009ac244b199900689436927684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeff090c8d64b7c9f8fd4a7a2c1c249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743d0d7e1dc4e99ac799edda7ed12a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d560db5954024bd9f0b3e658f16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2848d3cd2ce1470ba3bc0cae3418fab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef2dcd8be8c496ea2fe24cf19ecd0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo pre-entrenado para generar embeddings\n",
    "# Este modelo convierte texto en vectores numéricos de 384 dimensiones\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Convierte los fragmentos de texto en vectores numéricos (embeddings).\n",
    "    Estos vectores capturan el significado semántico del texto.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): lista de fragmentos de texto\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: matriz de embeddings, una fila por chunk\n",
    "    \"\"\"\n",
    "    # Codificar cada chunk en un vector de embedding\n",
    "    # show_progress_bar=True muestra una barra de progreso durante el procesamiento\n",
    "    return model.encode(chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store embeddings with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:19:31.447532Z",
     "iopub.status.busy": "2025-06-20T22:19:31.446816Z",
     "iopub.status.idle": "2025-06-20T22:19:31.497575Z",
     "shell.execute_reply": "2025-06-20T22:19:31.496496Z",
     "shell.execute_reply.started": "2025-06-20T22:19:31.447504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Construye un índice FAISS (base de datos vectorial) para búsqueda rápida.\n",
    "    FAISS permite encontrar embeddings similares de manera eficiente.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): matriz de embeddings (n_chunks x dimensión)\n",
    "        \n",
    "    Returns:\n",
    "        faiss.Index: índice FAISS configurado y poblado con los embeddings\n",
    "    \"\"\"\n",
    "    # Obtener la dimensión del embedding (384 para all-MiniLM-L6-v2)\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Crear un índice de búsqueda con distancia L2 (Euclidiana)\n",
    "    # IndexFlatL2 es una búsqueda exacta (sin aproximaciones)\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    \n",
    "    # Agregar los embeddings al índice\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:20:14.785103Z",
     "iopub.status.busy": "2025-06-20T22:20:14.784611Z",
     "iopub.status.idle": "2025-06-20T22:21:53.874292Z",
     "shell.execute_reply": "2025-06-20T22:21:53.872985Z",
     "shell.execute_reply.started": "2025-06-20T22:20:14.785072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5899c9232d174edaa02094ced7f8497e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ba25867f0c4b158b0fbf229e40a06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c136b44fb7bb4bf8a3cd7b7aa25bf882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d252ac5d9a424999ff9a3117428992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aec6999a83f46ab85122151bc8ba6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c821ea8263a41c8990aec17712189c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7850e84deb934d00892795df729b559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c3a278c1b4893a05f62998a7c443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd59e62f3cdd41bb80b7c08443ae231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27523318836d457b93a0dc4f4bf3ef7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a601715b39494978b4f6005fb371f958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a57473ce2343a9b6a651ee2f88d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "# Cargar credenciales de HuggingFace desde archivo JSON\n",
    "# Este token permite acceder a modelos privados o con cuota limitada\n",
    "with open(\"/kaggle/input/autenti/AUTH nn.json\", \"r\") as f:\n",
    "    token_data = json.load(f)\n",
    "\n",
    "HF_TOKEN = token_data[\"API_KEY\"]\n",
    "\n",
    "# Autenticarse en HuggingFace Hub\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Información del modelo a usar:\n",
    "# - 7 mil millones de parámetros\n",
    "# - Soporta múltiples idiomas: inglés, español, alemán, italiano, francés, portugués, etc.\n",
    "# - Datos de entrenamiento: Common Crawl, código abierto de GitHub, Wikipedia, Project Gutenberg\n",
    "#   Corpus multilingües (OSCAR, C4, The Pile), Q&A técnico, foros (Reddit, StackExchange)\n",
    "\n",
    "# Cargar el pipeline para generación de texto\n",
    "# Usando Nous-Hermes-2-Mistral-7B-DPO (o Mistral-7B-Instruct-v0.1 como alternativa)\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\")\n",
    "\n",
    "def retrieve_and_answer(query, chunks, index, embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Implementa el pipeline RAG (Retrieval-Augmented Generation):\n",
    "    1. Recupera los fragmentos más relevantes del documento\n",
    "    2. Usa el LLM para responder basado en el contexto recuperado\n",
    "    \n",
    "    Args:\n",
    "        query (str): pregunta del usuario\n",
    "        chunks (list): fragmentos de texto del documento\n",
    "        index (faiss.Index): índice FAISS para búsqueda\n",
    "        embeddings (np.ndarray): embeddings de los fragmentos\n",
    "        top_k (int): número de fragmentos relevantes a recuperar (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        str: respuesta generada por el modelo\n",
    "    \"\"\"\n",
    "    # Paso 1: Convertir la pregunta en un embedding\n",
    "    query_vec = model.encode([query])\n",
    "    \n",
    "    # Paso 2: Buscar los top_k fragmentos más similares en FAISS\n",
    "    # distances: distancias a los fragmentos encontrados\n",
    "    # indices: índices de los fragmentos más similares\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    \n",
    "    # Paso 3: Recuperar el texto de los fragmentos relevantes\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "    \n",
    "    # Paso 4: Construir el prompt combinando contexto + pregunta\n",
    "    context = \"\\n--------------------------------------------------\\n\".join(retrieved_texts)\n",
    "    prompt = f\"Answer this question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Paso 5: Generar respuesta usando el LLM\n",
    "    # max_new_tokens: limitar la longitud de la respuesta\n",
    "    # do_sample=True: usar muestreo (más creativo) en lugar de greedy decoding\n",
    "    response = qa_pipeline(prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:22:44.244429Z",
     "iopub.status.busy": "2025-06-20T22:22:44.243504Z",
     "iopub.status.idle": "2025-06-20T22:22:45.734170Z",
     "shell.execute_reply": "2025-06-20T22:22:45.732828Z",
     "shell.execute_reply.started": "2025-06-20T22:22:44.244378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9ab24f2d214f3db022199bf6874911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ruta al archivo PDF a procesar\n",
    "pdf_path = \"/kaggle/input/dataset/WWII_Key_Events_2pages.pdf\"\n",
    "\n",
    "# Ejecutar el pipeline RAG completo:\n",
    "\n",
    "# 1. Extraer texto del PDF\n",
    "text = extract_text_from_pdf_by_sentences(pdf_path)\n",
    "\n",
    "# 2. Dividir el texto en fragmentos manejables\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# 3. Generar embeddings para cada fragmento\n",
    "embeddings = embed_chunks(chunks)\n",
    "\n",
    "# 4. Construir el índice FAISS para búsqueda rápida\n",
    "index = build_faiss_index(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:27:23.247957Z",
     "iopub.status.busy": "2025-06-20T22:27:23.247546Z",
     "iopub.status.idle": "2025-06-20T22:30:23.813381Z",
     "shell.execute_reply": "2025-06-20T22:30:23.811810Z",
     "shell.execute_reply.started": "2025-06-20T22:27:23.247909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e98ec63e594b18a8b8199dd44fc58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time in minutes: 3.0092994372049966\n",
      "Answer this question based on the context below:\n",
      "\n",
      "Context:\n",
      "Operation Barbarossa (1941)\n",
      "In June 1941, Nazi Germany launched Operation Barbarossa, a massive invasion of the Soviet\n",
      "Union. This marked a major escalation of the war and opened the Eastern Front, which became the\n",
      "largest and bloodiest theater of conflict in World War II. Despite early successes, German forces\n",
      "were eventually halted outside Moscow and faced fierce Soviet resistance, especially in Stalingrad. Pearl Harbor (1941)\n",
      "On December 7, 1941, the Japanese navy launched a surprise attack on the U.S. naval base at\n",
      "Pearl Harbor in Hawaii. The attack killed over 2,400 Americans and led the United States to declare\n",
      "war on Japan the following day. This event brought the United States into the global conflict, which\n",
      "shifted the balance of power among the Allied and Axis forces. Battle of Stalingrad (1942-1943)\n",
      "One of the turning points of the war, the Battle of Stalingrad saw Soviet forces defending the city\n",
      "from a brutal German siege.\n",
      "--------------------------------------------------\n",
      "Key Events of World War II\n",
      "Invasion of Poland (1939)\n",
      "On September 1, 1939, Germany invaded Poland, triggering the beginning of World War II. This act\n",
      "of aggression led Britain and France to declare war on Germany just two days later. The German\n",
      "army used a military strategy known as 'blitzkrieg', or lightning war, involving rapid attacks with tanks\n",
      "and aircraft to overwhelm the Polish defenses. Poland was quickly defeated, and this marked the\n",
      "start of a devastating global conflict. Fall of France (1940)\n",
      "In May 1940, Germany launched an invasion of France and the Low Countries. Within six weeks,\n",
      "German forces had bypassed the heavily fortified Maginot Line and captured Paris. The swift defeat\n",
      "shocked the world and left Britain to stand alone against Nazi Germany for much of the next year. France was divided into an occupied zone and a nominally independent Vichy government that\n",
      "collaborated with the Nazis.\n",
      "--------------------------------------------------\n",
      "After months of fierce combat and enormous casualties, the Soviet\n",
      "Union encircled and defeated the German 6th Army. This defeat marked the beginning of a major\n",
      "Soviet offensive pushing westward. D-Day - Normandy Invasion (1944)\n",
      "On June 6, 1944, Allied forces launched the largest amphibious invasion in history, landing on the\n",
      "beaches of Normandy, France. Codenamed Operation Overlord, this marked the beginning of the\n",
      "liberation of Western Europe from Nazi control. Despite strong German resistance, the Allies\n",
      "successfully established a foothold and began pushing inland. Fall of Berlin and Hitler's Death (1945)\n",
      "By April 1945, Soviet troops had reached Berlin. After weeks of intense street fighting, the city fell. Adolf Hitler committed suicide in his bunker on April 30. Germany surrendered shortly after, bringing\n",
      "an end to the war in Europe. Atomic Bombs and Japan's Surrender (1945)\n",
      "In August 1945, the United States dropped atomic bombs on the Japanese cities of Hiroshima and\n",
      "Nagasaki.\n",
      "\n",
      "Question: What caused the United States to enter World War II?\n",
      "Answer: The attack on the U.S. naval base at Pearl Harbor by the Japanese navy caused the United States to declare war on Japan and enter World War II.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Pregunta a responder basada en el documento PDF\n",
    "question = \"What caused the United States to enter World War II?\"\n",
    "\n",
    "# Registrar el tiempo de inicio\n",
    "initial = time.time()\n",
    "\n",
    "# Ejecutar el pipeline RAG para obtener la respuesta\n",
    "answer = retrieve_and_answer(question, chunks, index, embeddings)\n",
    "\n",
    "# Registrar el tiempo final\n",
    "final = time.time()\n",
    "\n",
    "# Mostrar el tiempo de ejecución en minutos\n",
    "print(\"time in minutes:\", (final-initial) / 60)\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "1. Ask 5 questions and evaluate model answers qualitatively. Add brief comments.\n",
    "\n",
    "2. Calculate BLEU score for the answers. Comment on the results.\n",
    "\n",
    "3. Upload your own PDF, ask 5 new questions, and comment on the new answers.\n",
    "\n",
    "4. Calculate BLEU score again and compare it with the previous results."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7642611,
     "sourceId": 12135899,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7686248,
     "sourceId": 12211360,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
