{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfadaf7a-04f6-4f1e-8cbd-e15dfe988e03",
   "metadata": {},
   "source": [
    "# Week 1 - Stable Diffusion \n",
    "\n",
    "Stable Diffusion is a powerful text-to-image generative model developed by [Stability AI](https://stability.ai/). It uses a type of deep learning called diffusion models to generate highly realistic images from textual descriptions (called prompts). Unlike earlier models that were often slow or required massive hardware, Stable Diffusion is designed to be efficient and open-source, allowing individuals to run it on consumer-grade GPUs.\n",
    "\n",
    "**Key features include:**\n",
    "\n",
    "- üî§ **Text-to-Image Generation:** You input a sentence like ‚Äúa cyberpunk city at night,‚Äù and it generates a unique image based on that.\n",
    "\n",
    "- üß† **Latent Diffusion:** Instead of working directly in pixel space, it operates in a compressed ‚Äúlatent‚Äù space, which makes it faster and more memory-efficient.\n",
    "\n",
    "- üõ†Ô∏è **Customizability:** Users can fine-tune it or use tools like ControlNet, LoRA, and DreamBooth to guide image generation.\n",
    "\n",
    "- üåç **Open Source:** Anyone can download and run it, sparking a large community of artists, developers, and hobbyists.\n",
    "\n",
    "\n",
    "**Fig 1. Applications of Computer Vision** - [Stable Diffusion Explanation](https://www.youtube.com/watch?v=QdRP9pO89MY)\n",
    "<img src=\"Computer Vision Applications.png\" alt=\"Computer Vision\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0391427a-1638-4338-8712-028d65c929c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.30.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\usuario\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\usuario\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.33.1)\n",
      "Requirement already satisfied: diffusers in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.32.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\USUARIO\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub transformers accelerate torch torchvision imageio diffusers json imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9b28a-f270-4e88-acd8-d01c51a2f5f2",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) is a company and open-source community that builds tools for natural language processing (NLP), machine learning, and AI research. It‚Äôs best known for developing the Transformers library, which provides access to powerful pre-trained models like BERT, GPT, T5, RoBERTa, Stable Diffusion and many others.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "- üß† **Transformers Library:** Easy-to-use interface for working with state-of-the-art models for tasks like text classification, translation, question answering, and more.\n",
    "\n",
    "- üåê **Model Hub:** A massive online repository ([https://huggingface.co](https://huggingface.co)) where you can find, upload, and share thousands of pre-trained models.\n",
    "\n",
    "- üõ†Ô∏è **Trainer API:** High-level API for training and fine-tuning models with just a few lines of code.\n",
    "\n",
    "- ü§ó **Community & Open Source:** Actively maintained by a huge community of researchers, developers, and enthusiasts.\n",
    "\n",
    "- üì¶ **Integration:** Works seamlessly with PyTorch, TensorFlow, and JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387146d5-537c-4eaa-92c3-9700ffbc67f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../AUTH.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;66;03m#Create your file as AUTH.json -> {\"API_KEY\":\"My API KEY\"}\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../AUTH.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     creds \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../AUTH.json'"
     ]
    }
   ],
   "source": [
    "import json #Create your file as AUTH.json -> {\"API_KEY\":\"My API KEY\"}\n",
    "with open('../AUTH.json', 'r') as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc70fe-3854-45c3-b75c-9400a5121b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Put here your personal token -> login(\"MY_API_KEY\")\n",
    "login(creds[\"MY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ed34e-3982-443f-90e6-435167096613",
   "metadata": {},
   "source": [
    "## Stable Diffusion v1.5\n",
    "\n",
    "[**Stable Diffusion v1.5**](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) is a latent text-to-image diffusion model capable of generating photo-realistic images from textual descriptions. It was fine-tuned from the v1.2 checkpoint using 595,000 steps at a resolution of 512x512 on the \"laion-aesthetics v2 5+\" dataset. To enhance classifier-free guidance sampling, 10% of the text-conditioning was dropped during training.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Features\n",
    "\n",
    "- **Model Type:** Latent Diffusion Model (LDM)  \n",
    "- **Text Encoder:** CLIP ViT-L/14  \n",
    "- **Architecture Components:** Variational Autoencoder (VAE), U-Net, and a fixed text encoder  \n",
    "- **Training Data:** Images from the \"laion-aesthetics v2 5+\" dataset  \n",
    "- **License:** CreativeML Open RAIL-M  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- **Biases:** The model may reflect societal biases present in the training data (e.g., gender, race, age).  \n",
    "- **NSFW Content:** Capable of generating not safe for work (NSFW) or offensive content, including nudity or violence.  \n",
    "- **Hallucinations:** It may generate inaccurate or unrealistic representations, especially for abstract prompts.  \n",
    "- **No Real-World Understanding:** The model does not understand the world; it only generates patterns learned from data.  \n",
    "- **Ethical Risks:** Potential misuse includes deepfakes, misinformation, or offensive imagery.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú License: CreativeML Open RAIL-M\n",
    "\n",
    "- **Permissive Use:** Free for commercial and non-commercial use.  \n",
    "- **Restrictions:**  \n",
    "  - You cannot use the model to deliberately create or disseminate harmful, illegal, or offensive content.  \n",
    "  - You must credit the original creators when publishing outputs or derived models.  \n",
    "- **Modifications:** You can modify the model, but redistribution must include the same license terms.\n",
    "\n",
    "> This license aims to balance open access with responsible use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148c996-4194-4be9-9dc4-22ee44279936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d9de0963d478280624a8bf9991ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6ecb6d08f64059bf075a631ff7d30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efdc7d505124adab6fba55bd29e6501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a76f8ad53e24f91a92d1eb04b50e2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c07e9b0b144673a986fe06c240fa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6bd7e0ef2476692ba6659d84ad804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595214fb6cb5459e93fade0b7952a557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d9252208474f1886bc182eda7c76b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d8d7c8cc66426396c541e4f2f56a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60621ec65b7441349a80c4924cd963ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7089ab7053d460f815caef78a940bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d507a9ffd84025aa0adbda491cb31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c37480353a54ecdb1eeec9229b61ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50c1b34b6ba43dd8323c362f4202511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c092faf10e04978be15cc54a176c6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076ed4d8e7b04fe083fdf65a4b3548b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe_1 = StableDiffusionPipeline.from_pretrained(\n",
    "    \"sd-legacy/stable-diffusion-v1-5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f33fef-bc57-4f73-9dc2-31058f2c04ae",
   "metadata": {},
   "source": [
    "## Applications of Stable Diffusion\n",
    "\n",
    "1. **Text-to-Image Generation**  \n",
    "Turn a prompt into a unique image.\n",
    "\n",
    "*Example:*  \n",
    "Prompt ‚Üí \"A futuristic city at sunset\"  \n",
    "‚Üí Generates a visually rich image of that scene.\n",
    "\n",
    "2. **Image-to-Image Transformation (img2img)**  \n",
    "Modify existing images based on new prompts.\n",
    "\n",
    "*Example:*  \n",
    "Turn a sketch into a detailed artwork.\n",
    "\n",
    "3. **Inpainting (Image Editing)**  \n",
    "Fill in or edit specific parts of an image using a prompt.\n",
    "\n",
    "*Example:*  \n",
    "Remove objects, change backgrounds, or \"paint\" missing parts.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea8fa19a-b2ed-48ee-8d93-84cd33f7c396",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 1. Text-to-Image Generation ‚Äì My Golden Retriever üêï\n",
    "\n",
    "One warm evening, as the sun dipped low on the horizon, I imagined a beautiful scene and typed into the AI:  \n",
    "*\"a golden retriever running in a field during sunset, 4k photo, motion blur, natural lighting.\"*\n",
    "\n",
    "Almost instantly, an image appeared‚Äîa golden retriever sprinting joyfully across a vast open field. The fading sunlight cast a warm glow over the scene, highlighting the dog‚Äôs flowing fur. Motion blur captured the speed and energy of his run, while the natural light bathed everything in soft, golden hues.\n",
    "\n",
    "With this simple prompt, I was able to bring to life a moment of pure happiness and freedom‚Äîcapturing not just a dog, but a feeling.\n",
    "\n",
    "Thanks to text-to-image generation, memories and dreams like this can be created again and again, just by describing them in words.\n",
    "\n",
    "<img src=\"golden retriever.png\" alt=\"Golden Retriever\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4412f-9d0e-4a11-8beb-a184f05a1639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f488bf55e3514f168ae8dcee773a6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate image\n",
    "prompt = \"a golden retriever running in a field during sunset, 4k photo, motion blur, natural lighting\"\n",
    "image = pipe_1(prompt, num_inference_steps=50).images[0]\n",
    "\n",
    "# Show and save image\n",
    "image.show()\n",
    "image.save(\"golden retriever.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e43db3-a166-490f-a3da-e65257e598bd",
   "metadata": {},
   "source": [
    "## Instruct-pix2pix\n",
    "\n",
    "**InstructPix2Pix** is an advanced image editing model that allows users to modify images based on natural language instructions. Developed by Tim Brooks, Aleksander Holynski, and Alexei A. Efros, it combines the capabilities of Stable Diffusion and GPT-3 to understand and apply edits described in plain English.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Features\n",
    "\n",
    "- **Instruction-Based Editing:** Modify images by providing textual prompts (e.g., \"make the sky sunset-colored\").  \n",
    "- **No Fine-Tuning Required:** Performs edits in a single forward pass without the need for additional training.  \n",
    "- **Rapid Processing:** Generates edited images in seconds.  \n",
    "- **Versatile Applications:** Suitable for tasks like object addition/removal, style changes, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- **Biases in Output:** May reflect societal biases present in the training data.  \n",
    "- **Limited Spatial Transformations:** Not ideal for significant spatial changes like altering camera angles or object positions.  \n",
    "- **Research-Oriented Weights:** The provided model weights are intended for research purposes and may not be suitable for commercial applications without additional safety measures.\n",
    "\n",
    "---\n",
    "\n",
    "### üìú License\n",
    "\n",
    "- **MIT License:** Permissive license allowing for use, modification, and distribution.  \n",
    "- **Use-Based Restrictions:** Prohibits use of the model and its derivatives for unlawful purposes or in violation of applicable laws.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f741e-4de6-4360-8380-5ccd2a63c966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae084b852f4415d95d9caaeb6d54576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c9389e44ea48e2b2d0d3f6a86c13ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "pipe_2 = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "    \"timbrooks/instruct-pix2pix\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "799a2a90-029e-4c53-bbd6-1622ac461abd",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Image-to-Image Transformation ‚Äì *I Wanna Sell My Wallet*\n",
    "\n",
    "I had a simple photo of my old leather wallet‚Äîworn but full of character. I wanted to make it look fresh and appealing for an online sale. So, I used an image-to-image model.\n",
    "\n",
    "By feeding the original photo and a prompt like *Extract the wallet of the image and put it in a white background‚Äù*, the AI transformed it beautifully. The wrinkles softened, the leather gleamed under bright light, and even the zipper looked pristine.\n",
    "\n",
    "This powerful tool helped me turn a casual snapshot into a professional-looking product photo, ready to attract buyers.\n",
    "\n",
    "With image-to-image editing, selling something online doesn‚Äôt mean you need expensive equipment or perfect photos‚Äîjust smart AI and a clear vision.\n",
    "\n",
    "<img src=\"wallet.jpeg\" alt=\"My Wallet\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea88862-d655-42f4-9717-85b599f2f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image to modify\n",
    "init_image = Image.open(\"wallet.jpeg\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"Extract the wallet of the image and put it in a white background\"\n",
    "\n",
    "# Run img2img\n",
    "image = pipe_2(prompt=prompt, image=init_image, strength=0.6, guidance_scale=7.5, num_inference_steps=50).images[0]\n",
    "\n",
    "# Save or show result\n",
    "image.save(\"wallet without background.jpg\")\n",
    "image.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd093c7c-5e90-465f-8770-895c2b6693ce",
   "metadata": {},
   "source": [
    "### The result \n",
    "<img src=\"wallet without background.jpg\" alt=\"My modified Wallet\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff91f5-b1ee-4360-9da3-13f26b810bef",
   "metadata": {},
   "source": [
    "## 2.1 Improving my prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242edee-8f7a-43d0-891e-a006344534d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685bb207578847c8881298de378eff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a28fe6538b24684b370541bd605221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load an image to modify\n",
    "init_image = Image.open(\"wallet.jpeg\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"Isolate the wallet and place it on a plain white background. Make it look like a professional product photo.\"\n",
    "\n",
    "# Run img2img\n",
    "image = pipe_2(prompt=prompt, image=init_image, strength=0.6, guidance_scale=7.5, num_inference_steps=50).images[0]\n",
    "\n",
    "# Save or show result\n",
    "image.save(\"wallet without background with a optimized prompt.jpg\")\n",
    "image.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec11bb95-1f55-4a55-a010-063822b02063",
   "metadata": {},
   "source": [
    "### The result \n",
    "<img src=\"wallet without background with a optimized prompt.jpg\" alt=\"wallet without background with a optimized prompt.jpg\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea9d7c-cd81-4ab6-b1cf-601f378b24e0",
   "metadata": {},
   "source": [
    "## üé• Creating My Futuristic City in a Video\n",
    "\n",
    "I dreamed of building a breathtaking vision of the future‚Äîa cityscape bathed in the gentle light of sunrise. To bring this idea to life, I used an AI-driven text-to-video model with the prompt:  \n",
    "*\"A futuristic city at sunrise, cinematic, high detail, 8k.\"*\n",
    "\n",
    "The AI generated a stunning video sequence showcasing towering skyscrapers with sleek designs, glowing softly in the early morning light. The cinematic angles and ultra-high resolution made every detail pop‚Äîthe reflections on glass, the subtle haze, and the vibrant colors of dawn.\n",
    "\n",
    "Watching the city come alive on screen felt like stepping into a sci-fi movie. This technology allowed me to transform a simple description into a dynamic, immersive experience‚Äîa vision of the future unfolding frame by frame.\n",
    "\n",
    "Thanks to AI-powered video generation, creating complex and vivid scenes is no longer limited by traditional tools or budgets. Now, imagination is truly the only limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93c874-5661-47ea-8fc3-8146542c13f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc04c11a8aa94053850e22586df927b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frame 1 - Prompt: A futuristic city at morning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce0525520e44bbb8d58153eea1df15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frame 2 - Prompt: A futuristic city at noon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417b2ee9c9c40c1bf7ef69c8c42ab91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frame 3 - Prompt: A futuristic city at evening\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9bc4d9c7f49008e02f42e45e076e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frame 4 - Prompt: A futuristic city at sunset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84d8e158c5d46f49efa17e8b6aad612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frame 5 - Prompt: A futuristic city at night\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7303dde3dd4948c59b80663b1fadf549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_6720\\3948555201.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  frame = imageio.imread(f\"frames/frame_{i:03d}.png\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Video guardado como smooth_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# Paso 4: Crear una carpeta para guardar las im√°genes\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "\n",
    "# 4. Imagen inicial desde texto\n",
    "initial_prompt = \"A futuristic city at sunrise, cinematic, high detail, 8k\"\n",
    "init_image = pipe_1(initial_prompt, strength=0.75, guidance_scale=7.5, num_inference_steps=50).images[0]\n",
    "init_image.save(\"frames/frame_000.png\")\n",
    "\n",
    "# 5. Prompts interpolados (puedes modificar el tema gradualmente)\n",
    "prompts = [\n",
    "    \"A futuristic city at morning\",\n",
    "    \"A futuristic city at noon\",\n",
    "    \"A futuristic city at evening\",\n",
    "    \"A futuristic city at sunset\",\n",
    "    \"A futuristic city at night\"\n",
    "]\n",
    "\n",
    "# 6. Generar frames fluidos con img2img\n",
    "prev_image = init_image\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generando frame {i+1} - Prompt: {prompt}\")\n",
    "    \n",
    "    # Convertir PIL a formato compatible\n",
    "    image = prev_image.resize((512, 512)).convert(\"RGB\")\n",
    "    \n",
    "    new_image = pipe_1(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        strength=0.6,  # Cambia la fuerza para m√°s/menos diferencia\n",
    "        guidance_scale=7.5\n",
    "    ).images[0]\n",
    "    \n",
    "    frame_name = f\"frames/frame_{i+1:03d}.png\"\n",
    "    new_image.save(frame_name)\n",
    "    \n",
    "    # Usar esta imagen como base para la siguiente\n",
    "    prev_image = new_image\n",
    "\n",
    "def interpolate_frames(img1, img2, steps=5):\n",
    "    # Convertir a PIL si es necesario\n",
    "    img1 = Image.fromarray(img1)\n",
    "    img2 = Image.fromarray(img2)\n",
    "    return [Image.blend(img1, img2, alpha=i/steps) for i in range(1, steps)]\n",
    "\n",
    "frames = []\n",
    "for i in range(len(prompts)):\n",
    "    frame1 = imageio.imread(f\"frames/frame_{i:03d}.png\")\n",
    "    frame2 = imageio.imread(f\"frames/frame_{i+1:03d}.png\")\n",
    "    \n",
    "    frames.append(frame1)\n",
    "    transition_frames = interpolate_frames(frame1, frame2, steps=5)\n",
    "    frames.extend([np.array(f) for f in transition_frames])\n",
    "\n",
    "# A√±adir el √∫ltimo frame\n",
    "frames.append(imageio.imread(f\"frames/frame_{len(prompts):03d}.png\"))\n",
    "\n",
    "imageio.mimsave(\"a_futuristic_city_video.mp4\", frames, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de34f1c-eef5-436a-a038-9392324c12b1",
   "metadata": {},
   "source": [
    "## Activities\n",
    "\n",
    "1. Create your own image with a nifty prompt (free topic)\n",
    "2. Take a picture with your picture and modified it with a optimized prompt (free topic) -> E.g. Changes of environments or style, remove background, etc...\n",
    "3. Make your own video with a sequence prompt (free topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5536b92-a637-41e5-8c4a-427410bfeb16",
   "metadata": {},
   "source": [
    "## üåê Other Resources about Stable Diffusion (Beyond Hugging Face)\n",
    "\n",
    "### üß† 1. [Stability AI (Official)](https://stability.ai)\n",
    "- Official creators of Stable Diffusion.\n",
    "- Offers research papers, blog posts, updates, and commercial tools.\n",
    "\n",
    "---\n",
    "\n",
    "### üñºÔ∏è 2. [DreamStudio](https://dreamstudio.ai)\n",
    "- Web-based tool developed by Stability AI.\n",
    "- Generate images from text with adjustable parameters (steps, guidance scale, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ 3. GitHub Repositories\n",
    "- [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion): The original open-source release.\n",
    "- [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui): Popular web UI for running Stable Diffusion locally with extensive features.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö 4. Research Papers\n",
    "- [Original Paper on arXiv](https://arxiv.org/abs/2112.10752): *High-Resolution Image Synthesis with Latent Diffusion Models*.\n",
    "- Explore new innovations like SDXL, ControlNet, and LoRA via [arXiv.org](https://arxiv.org).\n",
    "\n",
    "---\n",
    "\n",
    "### üåê 5. [Civitai](https://civitai.com)\n",
    "- Community platform for sharing and downloading custom models, LoRAs, embeddings, and fine-tuned styles for Stable Diffusion.\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ 6. [Runway ML](https://runwayml.com)\n",
    "- User-friendly platform for creative AI tools.\n",
    "- Offers text-to-image, image-to-video, and more, including Stable Diffusion-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ 7. Community Forums\n",
    "- [r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/)\n",
    "- [r/StableDiffusionAI](https://www.reddit.com/r/StableDiffusionAI/)\n",
    "- Many GitHub and tool websites link to their own Discord servers for support and discussion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e707e4-f042-4a1f-a93a-20e65992c08c",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Stability AI, ‚ÄúStability.Ai,‚Äù Stability.Ai, 2024. https://stability.ai/\n",
    "2. Hugging Face, ‚ÄúHugging Face ‚Äì On a mission to solve NLP, one commit at a time.,‚Äù huggingface.co, 2024. https://huggingface.co/\n",
    "3. Render Realm, ‚ÄúStable Diffusion explained (in less than 10 minutes),‚Äù YouTube, Mar. 29, 2024. https://www.youtube.com/watch?v=QdRP9pO89MY (accessed Nov. 26, 2024).\n",
    "4. ‚Äútimbrooks/instruct-pix2pix ¬∑ Hugging Face,‚Äù Huggingface.co, 2025. https://huggingface.co/timbrooks/instruct-pix2pix (accessed Jun. 03, 2025).\n",
    "‚Äå"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
